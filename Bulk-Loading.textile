There are a number of configuration options and tools that make ingesting large amounts of graph data into Titan more efficient. Such ingestion is referred to as _bulk loading_ in contrast to the default _transactional loading_ where small amounts of data are added through individual transactions.
There are a number of use cases for bulk loading data into Titan, including:

* Introducing Titan into an existing environment with existing data and migrating or duplicating this data into a new Titan cluster.
* Using Titan as an end point of an "ETL":http://en.wikipedia.org/wiki/Extract,_transform,_load process.
* Adding an existing or external graph datasets (e.g. publicly available "RDF datasets":http://linkeddata.org/) to a running Titan cluster
* Updating a Titan graph with results from a graph analytics job.

This page describes configuration options and tools that make bulk loading more efficient in Titan. Please observe the limitations and assumptions for each option carefully before proceeding to avoid data loss or data corruption.

h2. Optimizing ID Allocation

h3. ID Block Size

Each newly added vertex or edge is assigned a unique id. Titan's id pool manager acquires ids in blocks for a particular Titan instance. The id block acquisition process is expensive because it needs carefully guarantee unique assignment of blocks. Increasing @ids.block-size@ reduces the number of acquisitions but potentially leaves many ids unassigned and hence wasted. For transactional workloads the default block size is reasonable, but during bulk loading vertices and edges are added much more frequently and in rapid succession. Hence, it is generally advisable to increase the block size by a factor of 10 or more depending on the number of vertices to be added per machine.

*Rule of thumb*: Set @ids.block-size@ to the number of vertices you expect to add per Titan instance per hour.

*Important:* All Titan instances MUST be configured with the same value for @ids.block-size@ to ensure proper id allocation. Hence, be careful to shut down all Titan instances prior to changing this value.

h3. ID Acquisition process

When id blocks are frequently allocated by many Titan instances in parallel, allocation conflicts between instances will inevitably arise and slow down the allocation process. In addition, the increased write load due to bulk loading may further slow down the process to the point where Titan considers it failed and throws an exception. There are three configuration options that can be tuned to avoid this.

***

@storage.idauthority-wait-time@ configures the time in milliseconds the id pool manager waits for an id block application to be acknowledged by the storage backend. The shorter this time, the more likely it is that an application will fail on a congested storage cluster.

*Rule of thumb*: Set this to the sum of the 95th percentile read and write times measured on the storage backend cluster under load.
*Important*: This value should be the same across all Titan instances.

***

@storage.idauthority-retries@ configures the number of times the id pool manager attempts to acquire an id block before giving up and throwing an exception.

*Rule of thumb*: Increase this value. The only downside of increasing it is that Titan will try for a long time on an unavailable storage backend cluster.

***

@ids.renew-timeout@ configures the number of milliseconds Titanâ€™s id pool manager will wait in total while attempting to acquire a new id block before failing.

*Rule of thumb*: Set this value to be larger than @2 x (storage.idauthority-retries) x (storage.idauthority-wait-time)@. The only downside of increasing it is that Titan will try for a long time on an unavailable storage backend cluster.